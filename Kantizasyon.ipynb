{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRapd-JySY4M",
        "outputId": "8610a01b-999a-430b-eb5f-4151e128cd7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers accelerate bitsandbytes torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import time\n",
        "\n",
        "# Model Adı (Daha küçük bir LLM seçildi, büyük modeller için VRAM'a dikkat edin)\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "# GPU kullanılabiliyorsa 'cuda', değilse 'cpu'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "## 1. FP32 (Normal) Model Yükleme ve Bellek Kullanımını Kontrol Etme\n",
        "print(f\"--- 1. Normal (FP32) Model Yükleniyor ---\")\n",
        "try:\n",
        "    # Modelin FP32 ağırlıklarıyla yükleneceği varsayılır (Bellekte yer kaplar)\n",
        "    model_fp32 = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float32,\n",
        "        low_cpu_mem_usage=True\n",
        "    ).to(device)\n",
        "\n",
        "    # Bellek kontrolü (Sadece GPU'daysa anlamlıdır)\n",
        "    if device == 'cuda':\n",
        "        fp32_mem = torch.cuda.memory_allocated(device) / (1024**3)\n",
        "        print(f\"FP32 Model GPU Bellek Kullanımı: {fp32_mem:.2f} GB\")\n",
        "        del model_fp32\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"FP32 model yüklenirken hata oluştu (normaldir eğer yetersiz VRAM varsa): {e}\")\n",
        "\n",
        "\n",
        "# --- 2. Kantizasyon Ayarlarını Tanımlama (4-bit/INT4) ---\n",
        "# BitsAndBytesConfig ile 4-bit kantizasyon ayarları yapılır.\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,                   # Ağırlıkları 4-bit tamsayı (INT4) olarak yükle\n",
        "    bnb_4bit_quant_type=\"nf4\",           # 4-bit NormalFloat formatını kullan (genellikle daha iyi performans)\n",
        "    bnb_4bit_use_double_quant=True,      # Çift kantizasyon (ek bellek tasarrufu)\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16 # Hesaplamaları bfloat16'da yap (doğruluk için)\n",
        ")\n",
        "\n",
        "print(\"\\n--- 2. Kantize Edilmiş (INT4) Model Yükleniyor ---\")\n",
        "\n",
        "start_time = time.time()\n",
        "# Modeli kantizasyon ayarlarıyla yükleme\n",
        "model_quantized = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\" # Mümkün olan en verimli şekilde GPU/CPU'ya dağıt\n",
        ")\n",
        "end_time = time.time()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "print(f\"INT4 Model Yükleme Süresi: {end_time - start_time:.2f} saniye\")\n",
        "\n",
        "## 3. Kantize Modelin Bellek Kullanımını Kontrol Etme\n",
        "if device == 'cuda':\n",
        "    quant_mem = torch.cuda.memory_allocated(device) / (1024**3)\n",
        "    print(f\"INT4 Model GPU Bellek Kullanımı: {quant_mem:.2f} GB\")\n",
        "    # FP32 belleğe göre çok daha küçük bir değer görmelisiniz.\n",
        "    print(f\"Bellek Tasarrufu Oranı (yaklaşık): {fp32_mem / quant_mem:.2f} kat\")\n",
        "\n",
        "# --- 4. Kantize Model ile Çıkarım (Inference) Yapma ---\n",
        "prompt = \"Kantizasyon nedir ve neden önemlidir? Cevap: \"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "print(\"\\n--- 4. Kantize Model ile Çıkarım Yapılıyor ---\")\n",
        "output_start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs = model_quantized.generate(**inputs, max_new_tokens=100)\n",
        "output_end_time = time.time()\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"Çıkarım Süresi: {output_end_time - output_start_time:.2f} saniye\")\n",
        "print(\"\\nModel Cevabı:\")\n",
        "print(\"--------------------------------------------------\")\n",
        "print(response)\n",
        "print(\"--------------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_3dC6EfSh3j",
        "outputId": "2b2ae8bd-8f95-4335-d3ce-dc55b75865fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Normal (FP32) Model Yükleniyor ---\n",
            "FP32 Model GPU Bellek Kullanımı: 4.83 GB\n",
            "\n",
            "--- 2. Kantize Edilmiş (INT4) Model Yükleniyor ---\n",
            "INT4 Model Yükleme Süresi: 3.59 saniye\n",
            "INT4 Model GPU Bellek Kullanımı: 1.45 GB\n",
            "Bellek Tasarrufu Oranı (yaklaşık): 3.34 kat\n",
            "\n",
            "--- 4. Kantize Model ile Çıkarım Yapılıyor ---\n",
            "Çıkarım Süresi: 12.16 saniye\n",
            "\n",
            "Model Cevabı:\n",
            "--------------------------------------------------\n",
            "Kantizasyon nedir ve neden önemlidir? Cevap: İlk olarak, Kantizasyon, İngilizce dili ile ilgili olarak, İngilizce dili ile ilgili olarak, İngilizce dili ile ilgili olarak, İngilizce dili ile ilgili olarak, İngilizce dili ile ilgili olarak, İng\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 1. Modellerin Tanımlanması (Teacher & Student)\n",
        "# -----------------------------------------------------\n",
        "\n",
        "# Giriş boyutu (örneğin, bir resimdeki özellik vektörü)\n",
        "INPUT_SIZE = 784\n",
        "# Çıkış boyutu (sınıf sayısı)\n",
        "NUM_CLASSES = 10\n",
        "# Öğrenci modelin gizli katman boyutu (küçük)\n",
        "STUDENT_HIDDEN = 64\n",
        "# Öğretmen modelin gizli katman boyutu (büyük)\n",
        "TEACHER_HIDDEN = 256\n",
        "\n",
        "class TeacherModel(nn.Module):\n",
        "    \"\"\"Büyük, iyi eğitilmiş model (Daha yüksek kapasite)\"\"\"\n",
        "    def __init__(self):\n",
        "        super(TeacherModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(INPUT_SIZE, TEACHER_HIDDEN)\n",
        "        self.fc2 = nn.Linear(TEACHER_HIDDEN, NUM_CLASSES)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "class StudentModel(nn.Module):\n",
        "    \"\"\"Küçük, hızlı model (Daha düşük kapasite)\"\"\"\n",
        "    def __init__(self):\n",
        "        super(StudentModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(INPUT_SIZE, STUDENT_HIDDEN)\n",
        "        self.fc2 = nn.Linear(STUDENT_HIDDEN, NUM_CLASSES)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 2. Distillation Kayıp Fonksiyonu (Daha önceki yanıttan)\n",
        "# -----------------------------------------------------\n",
        "\n",
        "class DistillationLoss(nn.Module):\n",
        "    def __init__(self, temperature=4.0, alpha=0.7):\n",
        "        super(DistillationLoss, self).__init__()\n",
        "        self.T = temperature\n",
        "        self.alpha = alpha\n",
        "        self.hard_criterion = nn.CrossEntropyLoss()\n",
        "        self.kl_div = nn.KLDivLoss(reduction='batchmean')\n",
        "\n",
        "    def forward(self, student_logits, labels, teacher_logits):\n",
        "        # 1. Sert Etiket Kaybı\n",
        "        loss_hard = self.hard_criterion(student_logits, labels)\n",
        "\n",
        "        # 2. Yumuşak Etiket Kaybı (KL Divergence)\n",
        "        p_t = F.softmax(teacher_logits / self.T, dim=1)\n",
        "        log_p_s = F.log_softmax(student_logits / self.T, dim=1)\n",
        "        loss_soft = self.kl_div(log_p_s, p_t) * (self.T * self.T)\n",
        "\n",
        "        # 3. Toplam Distillation Kaybı\n",
        "        loss = self.alpha * loss_hard + (1 - self.alpha) * loss_soft\n",
        "\n",
        "        return loss, loss_hard.item(), loss_soft.item()\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# 3. Öğrenci Eğitim Döngüsü\n",
        "# -----------------------------------------------------\n",
        "\n",
        "# Model ve Parametrelerin Başlatılması\n",
        "teacher_model = TeacherModel()\n",
        "student_model = StudentModel()\n",
        "\n",
        "# **Önemli Not:** Gerçek bir senaryoda, TeacherModel önceden eğitilmiş ve sabitlenmiştir!\n",
        "# Burada, örnek için sadece StudentModel eğitilir.\n",
        "teacher_model.eval() # Öğretmen modeli değerlendirme moduna ayarla (Eğitilmeyecek)\n",
        "\n",
        "kd_loss_fn = DistillationLoss(temperature=5.0, alpha=0.6)\n",
        "optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
        "\n",
        "# Eğitim Parametreleri\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 64\n",
        "NUM_BATCHES = 20 # Veri kümesi simülasyonu\n",
        "\n",
        "print(f\"Öğrenci Gizli Boyut: {STUDENT_HIDDEN}, Öğretmen Gizli Boyut: {TEACHER_HIDDEN}\")\n",
        "print(f\"Distillation Sıcaklığı (T): {kd_loss_fn.T}, Alpha (α): {kd_loss_fn.alpha}\\n\")\n",
        "\n",
        "student_model.train() # Öğrenci modeli eğitim moduna ayarla\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    # Veri Kümesi Simülasyonu\n",
        "    for batch_idx in range(NUM_BATCHES):\n",
        "\n",
        "        # Rastgele Veri Üretimi\n",
        "        inputs = torch.randn(BATCH_SIZE, INPUT_SIZE)\n",
        "        labels = torch.randint(0, NUM_CLASSES, (BATCH_SIZE,))\n",
        "\n",
        "        # Adım 1: Öğretmen Çıktısını Al (Sabitlenmiş)\n",
        "        with torch.no_grad(): # Öğretmen modelin gradyanları hesaplanmaz\n",
        "            teacher_logits = teacher_model(inputs)\n",
        "\n",
        "        # Adım 2: Öğrenci Çıktısını Al\n",
        "        student_logits = student_model(inputs)\n",
        "\n",
        "        # Adım 3: Kaybı Hesapla (KD Loss)\n",
        "        total_loss, hard_loss, soft_loss = kd_loss_fn(\n",
        "            student_logits,\n",
        "            labels,\n",
        "            teacher_logits\n",
        "        )\n",
        "\n",
        "        # Adım 4: Geri Yayılım ve Optimizasyon\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += total_loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / NUM_BATCHES\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Ortalama KD Kaybı: {avg_loss:.4f} | Son Soft Loss: {soft_loss:.4f} | Son Hard Loss: {hard_loss:.4f}\")\n",
        "\n",
        "print(\"\\nEğitim tamamlandı. Öğrenci modeli artık öğretmen bilgisi ile eğitilmiştir.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfXW0Y-dTAFD",
        "outputId": "33020327-2792-4288-f0e3-5b2669696c5a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Öğrenci Gizli Boyut: 64, Öğretmen Gizli Boyut: 256\n",
            "Distillation Sıcaklığı (T): 5.0, Alpha (α): 0.6\n",
            "\n",
            "Epoch 1/5 | Ortalama KD Kaybı: 1.4181 | Son Soft Loss: 0.0425 | Son Hard Loss: 2.3245\n",
            "Epoch 2/5 | Ortalama KD Kaybı: 1.4175 | Son Soft Loss: 0.0462 | Son Hard Loss: 2.3198\n",
            "Epoch 3/5 | Ortalama KD Kaybı: 1.4114 | Son Soft Loss: 0.0421 | Son Hard Loss: 2.2894\n",
            "Epoch 4/5 | Ortalama KD Kaybı: 1.4109 | Son Soft Loss: 0.0382 | Son Hard Loss: 2.3267\n",
            "Epoch 5/5 | Ortalama KD Kaybı: 1.4146 | Son Soft Loss: 0.0384 | Son Hard Loss: 2.3247\n",
            "\n",
            "Eğitim tamamlandı. Öğrenci modeli artık öğretmen bilgisi ile eğitilmiştir.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "# --- 1. Model ve Ağırlık Hazırlığı ---\n",
        "# Basit bir doğrusal katman (giriş 50, çıkış 10)\n",
        "layer = nn.Linear(50, 10)\n",
        "\n",
        "# Ağırlıkları rastgele başlatıyoruz\n",
        "# Ağırlık matrisi boyutu: [10, 50] = 500 parametre\n",
        "print(f\"Başlangıç ağırlık boyutu: {layer.weight.shape}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "\n",
        "# --- 2. Başlangıç Seyreklik Kontrolü ---\n",
        "def get_sparsity(model):\n",
        "    \"\"\"Modeldeki toplam sıfır ağırlıkların oranını hesaplar.\"\"\"\n",
        "    total_params = 0\n",
        "    zero_params = 0\n",
        "\n",
        "    # Sadece 'weight' ve 'bias' parametrelerini kontrol et\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
        "            # PyTorch'un budanmış modelleri, 'weight_orig' ve 'weight_mask' kullanır\n",
        "            weight = getattr(module, 'weight', None)\n",
        "\n",
        "            if weight is not None:\n",
        "                total_params += weight.numel()\n",
        "                zero_params += torch.sum(weight == 0).item()\n",
        "\n",
        "    if total_params == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return (zero_params / total_params) * 100\n",
        "\n",
        "initial_sparsity = get_sparsity(layer)\n",
        "print(f\"Budama öncesi seyreklik oranı: {initial_sparsity:.2f}%\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# --- 3. Yapılandırılmamış Budama Uygulaması ---\n",
        "\n",
        "# Bu işlem, L1 normuna (mutlak değer) göre en düşük\n",
        "# %30'luk kısmı budar (maskeyi sıfır yapar).\n",
        "# 'name': budanacak parametre, 'amount': budama oranı\n",
        "prune.l1_unstructured(layer, name='weight', amount=0.3)\n",
        "\n",
        "print(\"✅ Budama işlemi tamamlandı (L1 Unstructured, %30)\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# --- 4. Budama Sonrası Kontrol ---\n",
        "\n",
        "# Budama sonrasında katmanda iki yeni parametre oluşur:\n",
        "# 1. 'weight_orig': Orijinal ağırlık değerleri\n",
        "# 2. 'weight_mask': Budamayı uygulayan binary (0 veya 1) maske\n",
        "print(f\"Katman parametreleri: {list(layer.named_parameters())}\")\n",
        "\n",
        "# PyTorch, ileri yayılım (forward pass) sırasında 'weight_orig' ve 'weight_mask'\n",
        "# kullanarak dinamik olarak budanmış ağırlığı ('weight') oluşturur.\n",
        "# Oluşturulan 'weight' maske ile sıfırları içerir:\n",
        "final_sparsity = get_sparsity(layer)\n",
        "print(f\"Budama sonrası seyreklik oranı: {final_sparsity:.2f}%\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# --- 5. Budamayı Kalıcı Hale Getirme (Gerektiğinde) ---\n",
        "\n",
        "# Modelin daha sonra eğitilmesi veya dışa aktarılması (export) için\n",
        "# maske ve orijinal ağırlık değerlerini birleştirip kalıcı hale getirmeliyiz.\n",
        "# Bu işlem, orijinal ağırlıkları ve maskeyi kaldırarak **sıfırları**\n",
        "# doğrudan 'weight' parametresine yazar ve budama araçlarını temizler.\n",
        "# **DİKKAT:** Bu adımdan sonra budama geri alınamaz!\n",
        "\n",
        "# prune.remove(layer, 'weight')\n",
        "# print(\"Budama kalıcı hale getirildi ve budama yardımcıları temizlendi.\")\n",
        "# print(f\"Kalıcı hale getirilmiş parametreler: {list(layer.named_parameters())}\")\n",
        "# print(\"-\" * 30)\n",
        "\n",
        "# Budamadan sonra ağırlık boyutu hala [10, 50] olacaktır, ancak içindeki\n",
        "# değerlerin yaklaşık %30'u **tam olarak sıfırdır**.\n",
        "# Modelin bellekte kapladığı yer veya hesaplama (FLOPs) ancak bu model\n",
        "# seyrekleştirmeyi destekleyen bir formata dönüştürülüp (örneğin ONNX'te)\n",
        "# özel bir donanım/kütüphane üzerinde çalıştırılırsa azalır."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0gXWmwOTgGS",
        "outputId": "06742bcf-110d-464f-cfe4-0b60cf8efd85"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Başlangıç ağırlık boyutu: torch.Size([10, 50])\n",
            "------------------------------\n",
            "Budama öncesi seyreklik oranı: 0.00%\n",
            "------------------------------\n",
            "✅ Budama işlemi tamamlandı (L1 Unstructured, %30)\n",
            "------------------------------\n",
            "Katman parametreleri: [('bias', Parameter containing:\n",
            "tensor([-0.1332,  0.0016, -0.0939,  0.0789,  0.1242, -0.0854,  0.0027, -0.0797,\n",
            "        -0.1292, -0.0543], requires_grad=True)), ('weight_orig', Parameter containing:\n",
            "tensor([[-1.2679e-01,  1.2981e-01, -3.8986e-02, -1.3693e-01, -2.6731e-03,\n",
            "         -8.0254e-02, -7.1579e-02,  7.7183e-02, -1.0101e-03,  9.6338e-02,\n",
            "         -1.3199e-02, -2.3079e-02, -5.1178e-02, -8.1450e-02, -2.2775e-02,\n",
            "         -6.3556e-02, -8.9668e-02, -8.3085e-02,  8.9932e-02,  4.0637e-03,\n",
            "         -7.7488e-02,  1.0692e-01,  1.2273e-01, -1.3595e-01,  6.2558e-02,\n",
            "          3.5471e-02,  5.8022e-02, -9.7679e-02, -1.3083e-01,  1.0553e-01,\n",
            "         -2.4606e-03,  1.0615e-01, -1.2171e-01,  1.0275e-01,  7.0323e-02,\n",
            "         -2.5839e-03,  1.1090e-01,  5.1363e-02,  1.2664e-01, -2.4364e-03,\n",
            "          2.5169e-02, -1.7201e-02,  1.0588e-01,  4.0517e-03, -7.4752e-02,\n",
            "         -8.9278e-02,  6.4624e-02, -1.1972e-01,  1.2831e-01, -1.3817e-01],\n",
            "        [-2.8872e-02, -3.5706e-02, -1.5525e-02, -7.0263e-03,  1.2165e-01,\n",
            "          4.8898e-02,  1.6764e-02, -3.9946e-02,  5.4086e-02,  8.9984e-02,\n",
            "          1.7065e-03,  8.5704e-02,  1.3691e-01,  1.3503e-01,  1.2752e-01,\n",
            "          1.1644e-03,  8.2819e-02,  4.1293e-02, -8.0401e-02,  5.8497e-02,\n",
            "          9.3792e-03,  1.7392e-02,  1.0344e-01,  7.0008e-02,  9.1421e-02,\n",
            "         -5.1604e-03, -6.8818e-02,  6.9043e-02, -8.7711e-02, -2.1087e-02,\n",
            "         -8.8084e-02,  6.2339e-02,  1.2335e-01, -1.0904e-01, -1.0212e-01,\n",
            "         -1.3009e-01,  9.0713e-02, -4.3648e-02,  4.6827e-02, -1.1023e-01,\n",
            "         -1.0954e-01, -5.1519e-02, -1.8813e-02, -9.9334e-02,  1.3899e-01,\n",
            "          1.2475e-01,  1.3376e-01,  1.0702e-01, -6.7400e-02, -1.1837e-01],\n",
            "        [-2.3792e-02, -1.1418e-01,  1.0963e-01,  1.3931e-01,  8.9786e-03,\n",
            "         -5.4131e-02, -9.0275e-03, -1.0335e-01, -7.3684e-02,  1.1314e-01,\n",
            "          9.3447e-02, -1.2280e-01, -4.3607e-02, -4.3736e-02, -8.0257e-02,\n",
            "          2.9271e-02, -1.0264e-01, -8.4141e-02,  1.0515e-01,  1.1360e-01,\n",
            "         -9.9667e-02, -9.1818e-02,  1.0169e-01,  9.9078e-03, -1.3207e-01,\n",
            "         -1.0313e-01,  7.6835e-02,  9.4675e-03,  1.3531e-02,  6.2958e-02,\n",
            "         -7.6728e-02, -4.0842e-02,  1.1443e-01,  1.3290e-01, -2.9449e-02,\n",
            "         -1.0349e-01,  2.3515e-03, -5.3002e-02, -1.4003e-01, -9.5652e-02,\n",
            "          7.1837e-02, -1.0487e-02,  1.9810e-02,  5.1168e-02, -8.8783e-02,\n",
            "          5.9180e-03, -3.7051e-02,  9.3985e-02, -5.6008e-02, -1.3320e-02],\n",
            "        [-4.9896e-02, -7.0113e-02,  8.5758e-02, -2.8427e-02,  3.3919e-02,\n",
            "         -4.0022e-02, -1.2422e-01, -1.2750e-01, -2.3093e-02, -1.7667e-02,\n",
            "          1.9316e-02,  7.2895e-02,  4.4391e-02, -1.3742e-01, -1.0098e-01,\n",
            "          1.3636e-02, -3.2732e-02, -5.8666e-03, -1.0602e-01, -1.2967e-02,\n",
            "          3.8932e-02, -1.3764e-01, -2.3373e-02, -6.5388e-02, -4.6059e-02,\n",
            "          1.1135e-01,  9.9385e-02, -2.4370e-02, -4.9242e-02, -9.9142e-02,\n",
            "          6.2568e-02, -9.2795e-03, -1.3542e-01, -1.2476e-02, -6.0611e-02,\n",
            "          8.4097e-02, -5.8857e-02, -3.1201e-02,  8.9976e-02,  3.6290e-02,\n",
            "         -7.7708e-02, -8.5556e-02,  9.4934e-02,  7.6804e-02, -1.3133e-01,\n",
            "         -9.0342e-02,  1.1325e-02, -1.1001e-01, -1.0862e-01, -2.6138e-03],\n",
            "        [ 1.2910e-01,  2.3034e-02, -1.3626e-01,  1.3687e-02,  1.3233e-02,\n",
            "          1.0629e-01,  1.3795e-01, -9.5785e-02, -3.7396e-02, -9.7362e-02,\n",
            "          1.1914e-04,  8.1103e-02,  2.7757e-02,  1.1393e-01,  6.8012e-02,\n",
            "         -1.0035e-01, -1.3877e-01, -1.4035e-02, -3.8235e-03,  7.9114e-02,\n",
            "          5.1077e-02, -2.4075e-02,  2.2228e-02,  2.1138e-03, -6.7839e-02,\n",
            "         -7.3922e-02, -2.8698e-02,  6.5182e-02,  8.5837e-02, -9.7766e-02,\n",
            "          1.3683e-01, -3.9165e-02, -1.1867e-01,  7.2844e-02,  1.2762e-01,\n",
            "         -1.2051e-01, -1.2453e-01, -3.5811e-02,  1.1020e-02,  3.5913e-02,\n",
            "          8.0299e-02, -6.1137e-02,  1.1617e-01,  1.3125e-01, -1.1833e-01,\n",
            "          5.2002e-02,  9.2322e-02,  1.1832e-01,  2.7090e-02, -1.0379e-01],\n",
            "        [-1.1735e-01,  8.0476e-02, -8.1626e-02, -2.0005e-02, -5.4960e-02,\n",
            "          1.4361e-02, -7.3764e-03, -8.0999e-02,  8.9948e-02,  2.4584e-02,\n",
            "          6.3257e-02, -1.4776e-02,  2.5721e-02, -8.5277e-02, -1.2921e-01,\n",
            "          5.0194e-02, -8.4990e-02, -1.2001e-01,  1.3634e-01, -1.2525e-01,\n",
            "         -9.8775e-02,  8.7408e-02, -1.2933e-02, -5.1578e-02,  1.1806e-01,\n",
            "          1.0939e-01,  6.8593e-03,  1.1164e-01,  5.9881e-02,  3.5959e-02,\n",
            "         -6.4598e-02, -6.5961e-03,  1.3791e-01, -8.2992e-02,  8.3668e-02,\n",
            "          1.1907e-01, -1.1036e-01,  5.0776e-02,  1.4010e-01, -2.0679e-02,\n",
            "          7.0192e-03,  1.1659e-01, -9.8737e-02, -5.1965e-02,  8.2825e-02,\n",
            "          7.1629e-02, -1.0884e-01, -1.0210e-01, -7.6138e-02,  8.3247e-02],\n",
            "        [-2.2051e-04, -9.0756e-02, -3.7545e-02,  3.7305e-02, -6.8419e-02,\n",
            "         -3.3183e-02, -3.4599e-02,  9.0902e-02, -1.4052e-01, -3.9671e-02,\n",
            "          1.0904e-01, -4.7609e-03,  5.1012e-02, -6.7687e-02, -9.0476e-02,\n",
            "         -1.1602e-01, -5.0964e-02, -6.2372e-02,  1.2735e-01,  1.1553e-01,\n",
            "          7.8165e-02,  1.2949e-01,  1.3004e-01, -8.1108e-02, -2.5594e-02,\n",
            "         -8.6756e-02,  7.1955e-02, -1.3892e-01, -3.3329e-02, -1.0121e-01,\n",
            "         -1.2179e-01,  7.7284e-02,  8.6627e-02, -1.0317e-01,  7.7752e-02,\n",
            "          1.0842e-01,  5.0027e-02,  6.3662e-04, -1.3132e-01, -5.0874e-03,\n",
            "         -7.4822e-02,  1.1319e-01, -1.2033e-02,  1.0578e-01, -9.7352e-02,\n",
            "         -6.1028e-02,  1.0245e-01,  1.2644e-01,  1.1704e-01, -2.6194e-02],\n",
            "        [-1.1125e-01, -8.1749e-03,  9.0336e-02,  6.2866e-02,  6.2899e-03,\n",
            "         -2.9718e-02,  6.8977e-02,  9.9839e-02,  5.2555e-02,  7.2183e-02,\n",
            "          1.0618e-01,  5.9012e-02,  5.8823e-02, -2.9617e-02,  3.7674e-02,\n",
            "          1.3246e-01, -6.5087e-02, -1.6904e-02, -1.2137e-01, -6.0942e-02,\n",
            "          9.3475e-02,  5.1690e-02,  8.4268e-02,  9.6040e-02,  2.6256e-02,\n",
            "         -1.8819e-02,  5.4600e-02,  1.2851e-01, -3.3512e-02, -2.0739e-03,\n",
            "         -1.3118e-01, -1.1653e-01,  3.0313e-02,  9.0087e-02, -4.9991e-02,\n",
            "          5.1950e-02, -1.1201e-02, -6.1249e-04, -3.7044e-02, -3.8606e-03,\n",
            "          3.4426e-02,  5.7041e-03, -8.8469e-02, -8.0023e-02,  9.1319e-02,\n",
            "          5.9354e-02, -6.0065e-02, -1.4437e-02, -6.8916e-02,  9.1944e-02],\n",
            "        [ 1.7184e-02,  3.1792e-02,  1.3622e-01, -3.4393e-02,  7.2872e-02,\n",
            "         -7.4866e-02,  1.1657e-01,  1.0363e-01,  9.4418e-02,  2.6074e-02,\n",
            "         -7.7358e-02, -7.8332e-02,  5.2871e-03,  3.6459e-02,  8.8266e-02,\n",
            "          1.1764e-01, -4.7230e-02,  9.7545e-04,  1.4074e-01, -9.6163e-02,\n",
            "         -9.5754e-02,  8.1670e-02,  9.4719e-02, -9.1551e-03,  1.2828e-01,\n",
            "         -1.1717e-01, -5.6954e-02,  2.4671e-02, -1.2061e-02,  2.4569e-02,\n",
            "         -1.0857e-01,  3.1486e-02,  1.2702e-01,  4.6758e-02, -7.0497e-02,\n",
            "          9.2058e-02, -5.4812e-02,  9.8310e-02, -7.4030e-02,  8.6553e-02,\n",
            "         -1.1539e-01,  1.2068e-01,  4.2605e-02,  1.2922e-01,  3.0115e-02,\n",
            "          6.0589e-02,  1.3786e-01, -7.8623e-02,  8.4351e-02,  1.1865e-01],\n",
            "        [-2.9721e-02, -1.0065e-01, -1.0600e-01,  1.6794e-02, -5.4897e-02,\n",
            "         -1.6468e-02,  4.7099e-02,  9.2243e-02, -3.4810e-02,  5.0451e-02,\n",
            "         -1.0042e-01,  7.9915e-02, -1.0854e-01, -4.2715e-02,  1.0902e-01,\n",
            "         -1.0685e-01,  2.9663e-02,  6.2685e-02,  6.9118e-02,  1.1208e-01,\n",
            "         -7.1193e-02,  1.2046e-01,  9.2386e-02, -1.9697e-02, -5.1269e-02,\n",
            "          9.0886e-02, -4.7880e-02, -1.0312e-01,  1.3765e-01,  1.0118e-01,\n",
            "         -9.6530e-02,  2.9027e-02, -7.5446e-03,  2.1424e-02, -4.5997e-02,\n",
            "         -5.1533e-02, -1.0049e-01,  1.2725e-01, -2.0887e-02, -1.0921e-01,\n",
            "         -6.6981e-02,  9.0832e-02, -1.1898e-02,  6.4058e-02, -7.2866e-02,\n",
            "         -2.8750e-02, -5.5592e-02,  2.2040e-02,  6.2537e-02,  4.2621e-02]],\n",
            "       requires_grad=True))]\n",
            "Budama sonrası seyreklik oranı: 30.00%\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67cb9e01"
      },
      "source": [
        "Bu ilk kod hücresi, gerekli kütüphaneleri (transformers, accelerate, bitsandbytes, torch) yüklemek için kullanılır. Bu kütüphaneler genellikle büyük dil modellerini (LLM) yüklemek, hızlandırmak ve nicelleştirmek (quantization) için gereklidir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f68fb6ac"
      },
      "source": [
        "Bu kod hücresi, büyük dil modellerini (LLM) nicelleştirmenin (quantization) belleğe ve çıkarım hızına olan etkisini gösterir.\n",
        "\n",
        "### **Adım 1: Normal (FP32) Model Yükleme ve Bellek Kullanımını Kontrol Etme**\n",
        "\n",
        "*   Model, tam hassasiyetli (FP32) ağırlıklarla yüklenmeye çalışılır. Bu, genellikle daha fazla bellek gerektirir.\n",
        "*   Eğer GPU varsa, FP32 modelin ne kadar GPU belleği kullandığı hesaplanır. Yetersiz VRAM varsa bu adım hata verebilir, bu normaldir ve nicelleştirmenin neden gerekli olduğunu gösterir.\n",
        "\n",
        "### **Adım 2: Kantizasyon Ayarlarını Tanımlama (4-bit/INT4)**\n",
        "\n",
        "*   `BitsAndBytesConfig` kullanarak 4-bit nicelleştirme ayarları yapılır:\n",
        "    *   `load_in_4bit=True`: Model ağırlıklarını 4-bit tamsayı (INT4) olarak yükler.\n",
        "    *   `bnb_4bit_quant_type=\"nf4\"`: 4-bit NormalFloat formatını kullanır, genellikle daha iyi performans sağlar.\n",
        "    *   `bnb_4bit_use_double_quant=True`: Çift nicelleştirme kullanarak ek bellek tasarrufu sağlar.\n",
        "    *   `bnb_4bit_compute_dtype=torch.bfloat16`: Hesaplamaları bfloat16 (karma hassasiyet) formatında yapar, bu doğruluk ve hız arasında iyi bir denge sağlar.\n",
        "\n",
        "*   Model, tanımlanan `BitsAndBytesConfig` ayarlarıyla nicelleştirilmiş olarak yüklenir. `device_map=\"auto\"` parametresi, modelin parçalarını otomatik olarak GPU/CPU arasında dağıtarak en verimli şekilde yüklenmesini sağlar.\n",
        "*   Modelin yüklenme süresi ölçülür ve yazdırılır.\n",
        "\n",
        "### **Adım 3: Kantize Modelin Bellek Kullanımını Kontrol Etme**\n",
        "\n",
        "*   Eğer GPU varsa, nicelleştirilmiş modelin GPU bellek kullanımı hesaplanır. Bu değerin FP32 modele göre çok daha düşük olması beklenir.\n",
        "*   Bellek tasarrufu oranı (FP32 bellek / INT4 bellek) hesaplanarak nicelleştirmenin ne kadar etkili olduğu gösterilir.\n",
        "\n",
        "### **Adım 4: Kantize Model ile Çıkarım (Inference) Yapma**\n",
        "\n",
        "*   Önceden tanımlanmış bir `prompt` (istek) kullanılarak nicelleştirilmiş modelden bir yanıt üretilir.\n",
        "*   Çıkarım (inference) süresi ölçülür ve modelin cevabı ekrana yazdırılır.\n",
        "\n",
        "**Genel Amaç:** Bu kod, özellikle kısıtlı bellek kaynaklarına sahip cihazlarda (örneğin, küçük GPU'lar veya CPU'lar) büyük dil modellerini çalıştırmak için nicelleştirmenin (quantization) ne kadar faydalı olduğunu göstermektedir. Nicelleştirme, model boyutunu ve bellek kullanımını önemli ölçüde azaltırken, kabul edilebilir bir doğruluk seviyesi ile çıkarım hızını artırabilir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8649da24"
      },
      "source": [
        "Bu kod hücresi, makine öğreniminde **Bilgi Distilasyonu (Knowledge Distillation)** tekniğini uygulamak için bir örnek sunar. Bu teknik, büyük ve karmaşık bir \"Öğretmen Modelden\" (Teacher Model) öğrenerek daha küçük, daha hızlı ve daha verimli bir \"Öğrenci Model\" (Student Model) eğitmeyi amaçlar.\n",
        "\n",
        "### **1. Modellerin Tanımlanması (Teacher & Student)**\n",
        "\n",
        "*   **`INPUT_SIZE`**, **`NUM_CLASSES`**, **`STUDENT_HIDDEN`**, **`TEACHER_HIDDEN`** gibi sabitler tanımlanır. Bu, modelin giriş/çıkış boyutlarını ve gizli katmanların büyüklüğünü belirler.\n",
        "*   **`TeacherModel`** sınıfı, daha büyük bir gizli katmana (örneğin, `TEACHER_HIDDEN = 256`) sahip basit bir tam bağlı (fully connected) sinir ağı olarak tanımlanır. Bu, kapasitesi yüksek, iyi eğitilmiş bir modelin simülasyonudur.\n",
        "*   **`StudentModel`** sınıfı, daha küçük bir gizli katmana (örneğin, `STUDENT_HIDDEN = 64`) sahip benzer bir mimariye sahiptir. Bu, Öğretmen Modelden bilgi öğrenerek daha verimli hale gelecek olan modeldir.\n",
        "\n",
        "### **2. Distillation Kayıp Fonksiyonu (DistillationLoss)**\n",
        "\n",
        "*   **`DistillationLoss`** sınıfı, bilgi distilasyonu için özel bir kayıp fonksiyonu uygular. İki ana bileşeni vardır:\n",
        "    *   **`hard_criterion` (Sert Etiket Kaybı):** Öğrenci modelinin gerçek etiketlere (ground truth) göre ne kadar iyi performans gösterdiğini ölçen standart bir `CrossEntropyLoss`'tur. Bu, öğrencinin temel görevi öğrenmesini sağlar.\n",
        "    *   **`kl_div` (Yumuşak Etiket Kaybı / KL Divergence):** Öğrenci modelinin çıktılarının (logit'lerinin), öğretmen modelinin \"yumuşak hedefleri\"ne (softmax çıkışları) ne kadar benzediğini ölçer. `temperature` (sıcaklık) parametresi, öğretmen çıktılarının yumuşaklığını kontrol eder ve öğrencinin daha zengin bilgi öğrenmesine yardımcı olur.\n",
        "*   **`alpha`** parametresi, sert etiket kaybı ile yumuşak etiket kaybı arasındaki dengeyi ayarlar.\n",
        "*   `forward` metodu, her iki kayıp bileşenini hesaplar ve bunları `alpha` değerine göre ağırlıklandırarak toplam distilasyon kaybını döndürür.\n",
        "\n",
        "### **3. Öğrenci Eğitim Döngüsü**\n",
        "\n",
        "*   **`TeacherModel`** ve **`StudentModel`** örnekleri oluşturulur.\n",
        "*   **Önemli Not:** Gerçek bir senaryoda, Öğretmen Model genellikle önceden eğitilmiş ve ağırlıkları dondurulmuştur (`teacher_model.eval()` ve `with torch.no_grad()`). Bu örnekte de `teacher_model.eval()` ile değerlendirme moduna alınmış ve `torch.no_grad()` ile gradyan hesaplamaları devre dışı bırakılarak sabitlenmiş gibi davranılmıştır.\n",
        "*   **`DistillationLoss`** için sıcaklık (`temperature`) ve alfa (`alpha`) değerleri belirlenir.\n",
        "*   **`optimizer`**, sadece öğrenci modelinin parametrelerini güncelleyecek şekilde tanımlanır.\n",
        "*   Belirlenen `NUM_EPOCHS` (dönem) boyunca eğitim döngüsü çalışır:\n",
        "    *   Her dönemde, rastgele girişler (`inputs`) ve etiketler (`labels`) oluşturularak bir veri kümesi simüle edilir.\n",
        "    *   **Öğretmen çıktısı alınır:** `teacher_model` kullanılarak girişler üzerinde `teacher_logits` elde edilir. `torch.no_grad()` bloğu sayesinde bu adımda öğretmen modelinin ağırlıkları güncellenmez.\n",
        "    *   **Öğrenci çıktısı alınır:** `student_model` kullanılarak girişler üzerinde `student_logits` elde edilir.\n",
        "    *   **Distilasyon kaybı hesaplanır:** `kd_loss_fn` kullanılarak `student_logits`, `labels` ve `teacher_logits` ile toplam kayıp (`total_loss`), sert kayıp (`hard_loss`) ve yumuşak kayıp (`soft_loss`) hesaplanır.\n",
        "    *   **Geri yayılım ve optimizasyon:** `total_loss` geri yayılır ve `optimizer.step()` ile öğrenci modelinin ağırlıkları güncellenir.\n",
        "*   Her dönemin sonunda ortalama kayıp değerleri yazdırılır.\n",
        "\n",
        "**Genel Amaç:** Bu kod, bir öğrenci modelin, daha güçlü bir öğretmen modelden nasıl bilgi aktarımı yapabileceğini ve bu sayede kendi başına (direkt olarak etiketlerden öğrenmek yerine) daha iyi performans gösterebileceğini simüle eder. Bu, model sıkıştırma ve hızlandırma tekniklerinden biridir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4017e38f"
      },
      "source": [
        "Bu kod hücresi, derin öğrenme modellerinde **ağırlık budama (weight pruning)** tekniğini görselleştirmek için PyTorch'un `torch.nn.utils.prune` modülünü kullanır. Budama, modeldeki önemsiz ağırlıkları kaldırarak modelin boyutunu ve hesaplama yükünü azaltmayı hedefler.\n",
        "\n",
        "### **1. Model ve Ağırlık Hazırlığı**\n",
        "\n",
        "*   Basit bir doğrusal katman (`nn.Linear(50, 10)`) oluşturulur. Bu katman 50 giriş özelliğini 10 çıkış özelliğine eşler ve 500 ağırlık parametresine sahiptir (10 * 50).\n",
        "*   Başlangıç ağırlık matrisinin boyutu yazdırılır.\n",
        "\n",
        "### **2. Başlangıç Seyreklik Kontrolü (`get_sparsity`)**\n",
        "\n",
        "*   `get_sparsity` adında bir yardımcı fonksiyon tanımlanır. Bu fonksiyon, bir modeldeki toplam sıfır ağırlıkların yüzdesini hesaplar.\n",
        "*   `named_modules()` ile modeldeki tüm modüller gezilir ve `nn.Linear` veya `nn.Conv2d` gibi katmanlar tespit edilir.\n",
        "*   `weight.numel()` ile toplam ağırlık sayısı, `torch.sum(weight == 0)` ile de sıfır olan ağırlıkların sayısı bulunur.\n",
        "*   Budama öncesindeki seyreklik oranı (genellikle %0 veya sıfıra yakın bir değer) yazdırılır.\n",
        "\n",
        "### **3. Yapılandırılmamış Budama Uygulaması (`prune.l1_unstructured`)**\n",
        "\n",
        "*   `prune.l1_unstructured` fonksiyonu kullanılarak doğrusal katmanın ağırlıkları budanır.\n",
        "*   `layer`: Budama yapılacak katman (`nn.Linear` örneği).\n",
        "*   `name='weight'`: Budama yapılacak parametrenin adı (burada ağırlık matrisi).\n",
        "*   `amount=0.3`: Ağırlıkların %30'unun budanacağı anlamına gelir. `L1 normuna` (mutlak değer) göre en küçük %30'luk kısım sıfır yapılır.\n",
        "\n",
        "### **4. Budama Sonrası Kontrol**\n",
        "\n",
        "*   Budama işlemi tamamlandıktan sonra, PyTorch orijinal `weight` parametresini `weight_orig` ve `weight_mask` olmak üzere iki yeni parametreye dönüştürür:\n",
        "    *   `weight_orig`: Ağırlıkların orijinal değerlerini tutar.\n",
        "    *   `weight_mask`: Binary (0 veya 1) bir maske olup, budanan (sıfır olan) ağırlıkların konumunu gösterir.\n",
        "*   `layer.named_parameters()` çıktısı bu yeni parametreleri gösterir.\n",
        "*   Modelin ileri yayılımı (forward pass) sırasında, PyTorch otomatik olarak `weight_orig` ve `weight_mask`'i çarparak \"gerçek\" budanmış `weight` parametresini oluşturur.\n",
        "*   `get_sparsity` fonksiyonu tekrar çağrılarak budama sonrası seyreklik oranı hesaplanır. Bu değerin yaklaşık olarak %30 olması beklenir.\n",
        "\n",
        "### **5. Budamayı Kalıcı Hale Getirme (Gerektiğinde - Yorum Satırında)**\n",
        "\n",
        "*   `prune.remove(layer, 'weight')` komutu (şu anda yorum satırında), budama maskesini ve orijinal ağırlıklarını birleştirerek kalıcı hale getirir.\n",
        "*   Bu işlem, `weight_orig` ve `weight_mask`'i kaldırır ve ağırlık matrisindeki sıfırları doğrudan `weight` parametresine yazar.\n",
        "*   **Uyarı:** Bu adım geri alınamaz ve modelin ağırlıklarını kalıcı olarak değiştirir.\n",
        "\n",
        "**Genel Amaç:** Bu kod, model budamanın temel mekanizmalarını ve PyTorch'ta nasıl uygulanabileceğini gösterir. Budama, modelin daha az bellek kullanmasını ve daha hızlı çalışmasını sağlayarak dağıtım için daha uygun hale getirilmesine yardımcı olabilir, özellikle kısıtlı kaynaklara sahip cihazlarda."
      ]
    }
  ]
}